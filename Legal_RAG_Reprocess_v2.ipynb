{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üìö Legal RAG Reprocessing v2.0\n",
                "\n",
                "**Purpose:** Re-chunk existing cases with granular splitting (25+ chunks/case) for better search accuracy.\n",
                "\n",
                "**What this does:**\n",
                "1. Fetches all cases from your Supabase `cases` table\n",
                "2. Splits each judgment into smaller, more detailed chunks (~500 chars)\n",
                "3. Generates embeddings using `bge-small-en-v1.5` (384 dimensions)\n",
                "4. Clears old chunks and uploads new ones to `case_chunks`\n",
                "\n",
                "**Estimated time:** 2-4 hours for 4,688 cases on free Colab GPU"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 1Ô∏è‚É£ Install Dependencies\n",
                "!pip install -q supabase sentence-transformers tqdm"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 2Ô∏è‚É£ Configuration\n",
                "import os\n",
                "\n",
                "# @markdown Enter your Supabase credentials:\n",
                "SUPABASE_URL = \"https://vzqlwjibtujhrhjgwhhe.supabase.co\"  # @param {type:\"string\"}\n",
                "SUPABASE_KEY = \"\"  # @param {type:\"string\"}\n",
                "\n",
                "# Chunking settings - OPTIMIZED for detailed search\n",
                "CHUNK_SIZE = 500       # Characters per chunk (smaller = more granular)\n",
                "CHUNK_OVERLAP = 100    # Overlap between chunks (preserves context)\n",
                "MIN_CHUNK_SIZE = 100   # Minimum chunk size (skip tiny fragments)\n",
                "TARGET_CHUNKS_PER_CASE = 25  # Approximate target\n",
                "\n",
                "# Embedding model\n",
                "EMBEDDING_MODEL = \"BAAI/bge-small-en-v1.5\"  # 384 dimensions, matches your DB\n",
                "\n",
                "# Processing\n",
                "BATCH_SIZE = 32        # Embeddings per batch\n",
                "UPLOAD_BATCH = 100     # Chunks per upload batch\n",
                "\n",
                "print(\"‚úÖ Configuration loaded\")\n",
                "print(f\"   Chunk size: {CHUNK_SIZE} chars\")\n",
                "print(f\"   Target: ~{TARGET_CHUNKS_PER_CASE} chunks/case\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 3Ô∏è‚É£ Initialize Supabase & Model\n",
                "from supabase import create_client\n",
                "from sentence_transformers import SentenceTransformer\n",
                "import torch\n",
                "\n",
                "# Connect to Supabase\n",
                "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
                "print(\"‚úÖ Supabase connected\")\n",
                "\n",
                "# Load embedding model on GPU\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"üì± Device: {device}\")\n",
                "\n",
                "model = SentenceTransformer(EMBEDDING_MODEL, device=device)\n",
                "print(f\"‚úÖ Model loaded: {EMBEDDING_MODEL}\")\n",
                "print(f\"   Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 4Ô∏è‚É£ Fetch All Cases\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "def fetch_all_cases():\n",
                "    \"\"\"Fetch all cases from the cases table\"\"\"\n",
                "    all_cases = []\n",
                "    page_size = 1000\n",
                "    offset = 0\n",
                "    \n",
                "    print(\"üì• Fetching cases from Supabase...\")\n",
                "    \n",
                "    while True:\n",
                "        response = supabase.table(\"cases\").select(\n",
                "            \"id, hklii_id, case_name, neutral_citation, court, decision_date, full_text\"\n",
                "        ).range(offset, offset + page_size - 1).execute()\n",
                "        \n",
                "        if not response.data:\n",
                "            break\n",
                "            \n",
                "        all_cases.extend(response.data)\n",
                "        offset += page_size\n",
                "        print(f\"   Fetched {len(all_cases)} cases...\")\n",
                "        \n",
                "        if len(response.data) < page_size:\n",
                "            break\n",
                "    \n",
                "    print(f\"‚úÖ Total cases fetched: {len(all_cases)}\")\n",
                "    return all_cases\n",
                "\n",
                "cases = fetch_all_cases()"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 5Ô∏è‚É£ Improved Chunking Function\n",
                "import re\n",
                "from typing import List, Dict\n",
                "\n",
                "def clean_html(text: str) -> str:\n",
                "    \"\"\"Remove HTML tags and clean up text\"\"\"\n",
                "    if not text:\n",
                "        return \"\"\n",
                "    # Remove HTML tags\n",
                "    text = re.sub(r'<[^>]+>', ' ', text)\n",
                "    # Remove multiple spaces/newlines\n",
                "    text = re.sub(r'\\s+', ' ', text)\n",
                "    # Remove special characters but keep legal punctuation\n",
                "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]', '', text)\n",
                "    return text.strip()\n",
                "\n",
                "def detect_section_type(text: str, position: float) -> str:\n",
                "    \"\"\"Detect the type of legal section based on content and position\"\"\"\n",
                "    text_lower = text.lower()[:500]\n",
                "    \n",
                "    # Header/intro indicators\n",
                "    if position < 0.1:\n",
                "        if any(x in text_lower for x in ['background', 'introduction', 'before']):\n",
                "            return 'background'\n",
                "        return 'header'\n",
                "    \n",
                "    # Facts section\n",
                "    if any(x in text_lower for x in ['the facts', 'factual background', 'evidence shows', 'witness', 'testified']):\n",
                "        return 'facts'\n",
                "    \n",
                "    # Legal reasoning\n",
                "    if any(x in text_lower for x in ['held that', 'in my judgment', 'i am satisfied', 'the court finds', 'it is clear that']):\n",
                "        return 'reasoning'\n",
                "    \n",
                "    # Holding/disposition\n",
                "    if any(x in text_lower for x in ['order', 'accordingly', 'appeal dismissed', 'appeal allowed', 'judgment for']):\n",
                "        return 'holding'\n",
                "    \n",
                "    # Damages/compensation\n",
                "    if any(x in text_lower for x in ['damages', 'compensation', 'quantum', 'award']):\n",
                "        return 'damages'\n",
                "    \n",
                "    # Legal principles\n",
                "    if any(x in text_lower for x in ['principle', 'test is', 'duty of care', 'negligence', 'breach']):\n",
                "        return 'legal_principle'\n",
                "    \n",
                "    return 'general'\n",
                "\n",
                "def chunk_judgment(case: Dict) -> List[Dict]:\n",
                "    \"\"\"Split a judgment into granular chunks with metadata\"\"\"\n",
                "    text = clean_html(case.get('full_text', ''))\n",
                "    \n",
                "    if not text or len(text) < MIN_CHUNK_SIZE:\n",
                "        return []\n",
                "    \n",
                "    chunks = []\n",
                "    total_length = len(text)\n",
                "    \n",
                "    # Sliding window chunking with overlap\n",
                "    start = 0\n",
                "    chunk_index = 0\n",
                "    \n",
                "    while start < total_length:\n",
                "        # Calculate end position\n",
                "        end = min(start + CHUNK_SIZE, total_length)\n",
                "        \n",
                "        # Try to break at sentence boundary\n",
                "        if end < total_length:\n",
                "            # Look for sentence end within last 100 chars\n",
                "            search_start = max(start + CHUNK_SIZE - 100, start)\n",
                "            for punct in ['. ', '„ÄÇ', '\\n\\n']:\n",
                "                last_punct = text.rfind(punct, search_start, end + 50)\n",
                "                if last_punct > search_start:\n",
                "                    end = last_punct + len(punct)\n",
                "                    break\n",
                "        \n",
                "        chunk_text = text[start:end].strip()\n",
                "        \n",
                "        if len(chunk_text) >= MIN_CHUNK_SIZE:\n",
                "            position = start / total_length\n",
                "            \n",
                "            chunks.append({\n",
                "                'case_id': case.get('id'),\n",
                "                'hklii_id': case.get('hklii_id', ''),\n",
                "                'chunk_index': chunk_index,\n",
                "                'chunk_text': chunk_text,\n",
                "                'section_type': detect_section_type(chunk_text, position),\n",
                "                'case_name': case.get('case_name', ''),\n",
                "                'neutral_citation': case.get('neutral_citation', ''),\n",
                "                'court': case.get('court', ''),\n",
                "                'decision_date': case.get('decision_date', ''),\n",
                "            })\n",
                "            chunk_index += 1\n",
                "        \n",
                "        # Move start with overlap\n",
                "        start = end - CHUNK_OVERLAP if end < total_length else total_length\n",
                "    \n",
                "    return chunks\n",
                "\n",
                "# Test on first case\n",
                "if cases:\n",
                "    test_chunks = chunk_judgment(cases[0])\n",
                "    print(f\"‚úÖ Test chunking: {len(test_chunks)} chunks from first case\")\n",
                "    if test_chunks:\n",
                "        print(f\"   First chunk preview: {test_chunks[0]['chunk_text'][:100]}...\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 6Ô∏è‚É£ Clear Old Chunks (IMPORTANT!)\n",
                "#@markdown This will delete all existing chunks before re-processing.\n",
                "#@markdown Make sure you want to proceed!\n",
                "\n",
                "CONFIRM_DELETE = True  # @param {type:\"boolean\"}\n",
                "\n",
                "if CONFIRM_DELETE:\n",
                "    print(\"üóëÔ∏è Deleting old chunks...\")\n",
                "    \n",
                "    # Delete in batches to avoid timeout\n",
                "    deleted_total = 0\n",
                "    while True:\n",
                "        # Get batch of IDs\n",
                "        response = supabase.table(\"case_chunks\").select(\"id\").limit(1000).execute()\n",
                "        if not response.data:\n",
                "            break\n",
                "        \n",
                "        ids = [r['id'] for r in response.data]\n",
                "        supabase.table(\"case_chunks\").delete().in_(\"id\", ids).execute()\n",
                "        deleted_total += len(ids)\n",
                "        print(f\"   Deleted {deleted_total} chunks...\")\n",
                "    \n",
                "    print(f\"‚úÖ Cleared {deleted_total} old chunks\")\n",
                "else:\n",
                "    print(\"‚è≠Ô∏è Skipping deletion (will add to existing chunks)\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 7Ô∏è‚É£ Process All Cases (Main Loop)\n",
                "from tqdm.auto import tqdm\n",
                "import numpy as np\n",
                "\n",
                "def process_all_cases(cases: List[Dict]):\n",
                "    \"\"\"Process all cases: chunk, embed, and upload\"\"\"\n",
                "    \n",
                "    all_chunks = []\n",
                "    total_chunks = 0\n",
                "    cases_with_text = 0\n",
                "    \n",
                "    print(\"üì¶ Chunking all cases...\")\n",
                "    for case in tqdm(cases, desc=\"Chunking\"):\n",
                "        chunks = chunk_judgment(case)\n",
                "        if chunks:\n",
                "            all_chunks.extend(chunks)\n",
                "            cases_with_text += 1\n",
                "    \n",
                "    total_chunks = len(all_chunks)\n",
                "    avg_chunks = total_chunks / cases_with_text if cases_with_text > 0 else 0\n",
                "    \n",
                "    print(f\"\\n‚úÖ Chunking complete:\")\n",
                "    print(f\"   Cases with text: {cases_with_text}\")\n",
                "    print(f\"   Total chunks: {total_chunks}\")\n",
                "    print(f\"   Average chunks/case: {avg_chunks:.1f}\")\n",
                "    \n",
                "    # Generate embeddings in batches\n",
                "    print(f\"\\nüî¢ Generating embeddings for {total_chunks} chunks...\")\n",
                "    \n",
                "    embeddings = []\n",
                "    texts = [c['chunk_text'] for c in all_chunks]\n",
                "    \n",
                "    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"Embedding\"):\n",
                "        batch_texts = texts[i:i + BATCH_SIZE]\n",
                "        batch_embeddings = model.encode(\n",
                "            batch_texts,\n",
                "            batch_size=BATCH_SIZE,\n",
                "            show_progress_bar=False,\n",
                "            normalize_embeddings=True\n",
                "        )\n",
                "        embeddings.extend(batch_embeddings.tolist())\n",
                "    \n",
                "    print(f\"‚úÖ Generated {len(embeddings)} embeddings\")\n",
                "    \n",
                "    # Add embeddings to chunks\n",
                "    for i, chunk in enumerate(all_chunks):\n",
                "        chunk['embedding'] = embeddings[i]\n",
                "    \n",
                "    # Upload to Supabase in batches\n",
                "    print(f\"\\nüì§ Uploading {total_chunks} chunks to Supabase...\")\n",
                "    \n",
                "    uploaded = 0\n",
                "    errors = 0\n",
                "    \n",
                "    for i in tqdm(range(0, len(all_chunks), UPLOAD_BATCH), desc=\"Uploading\"):\n",
                "        batch = all_chunks[i:i + UPLOAD_BATCH]\n",
                "        \n",
                "        try:\n",
                "            supabase.table(\"case_chunks\").insert(batch).execute()\n",
                "            uploaded += len(batch)\n",
                "        except Exception as e:\n",
                "            errors += len(batch)\n",
                "            print(f\"\\n‚ö†Ô∏è Upload error at batch {i}: {str(e)[:100]}\")\n",
                "            # Try one by one for failed batch\n",
                "            for chunk in batch:\n",
                "                try:\n",
                "                    supabase.table(\"case_chunks\").insert(chunk).execute()\n",
                "                    uploaded += 1\n",
                "                    errors -= 1\n",
                "                except:\n",
                "                    pass\n",
                "    \n",
                "    print(f\"\\n\" + \"=\"*50)\n",
                "    print(f\"‚úÖ PROCESSING COMPLETE\")\n",
                "    print(f\"=\"*50)\n",
                "    print(f\"   Cases processed: {cases_with_text}\")\n",
                "    print(f\"   Chunks created: {total_chunks}\")\n",
                "    print(f\"   Chunks uploaded: {uploaded}\")\n",
                "    print(f\"   Errors: {errors}\")\n",
                "    print(f\"   Avg chunks/case: {avg_chunks:.1f}\")\n",
                "    \n",
                "    return uploaded\n",
                "\n",
                "# Run processing\n",
                "uploaded = process_all_cases(cases)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 8Ô∏è‚É£ Verify Results\n",
                "# Check new chunk count\n",
                "response = supabase.table(\"case_chunks\").select(\"id\", count=\"exact\").execute()\n",
                "print(f\"\\nüìä Verification:\")\n",
                "print(f\"   Total chunks in database: {response.count}\")\n",
                "\n",
                "# Check embedding count\n",
                "response2 = supabase.rpc(\"count_embeddings\").execute()\n",
                "# Alternative check\n",
                "sample = supabase.table(\"case_chunks\").select(\"id, hklii_id, chunk_index, section_type\").limit(5).execute()\n",
                "print(f\"\\nüìù Sample chunks:\")\n",
                "for row in sample.data:\n",
                "    print(f\"   {row['hklii_id']} - Chunk {row['chunk_index']} ({row['section_type']})\")\n",
                "\n",
                "# Check average chunks per case\n",
                "unique_cases = supabase.table(\"case_chunks\").select(\"hklii_id\").execute()\n",
                "unique_ids = set(r['hklii_id'] for r in unique_cases.data)\n",
                "print(f\"\\n   Unique cases: {len(unique_ids)}\")\n",
                "if unique_ids:\n",
                "    print(f\"   Avg chunks/case: {response.count / len(unique_ids):.1f}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## ‚úÖ Done!\n",
                "\n",
                "Your cases have been re-processed with more granular chunking.\n",
                "\n",
                "**Next steps:**\n",
                "1. Go to https://cabase.vercel.app/\n",
                "2. Test a search query\n",
                "3. Results should now be more detailed and accurate!\n",
                "\n",
                "**Expected improvement:**\n",
                "- Before: ~6 chunks/case (just summaries)\n",
                "- After: ~25 chunks/case (full judgment details)\n",
                "- Search can now find specific facts, damages, legal principles"
            ],
            "metadata": {}
        }
    ]
}